{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence - CA3\n",
    "## Hossein Soltanloo - 810195407"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the Data\n",
    "In order to clean the raw data used as training text, we follow a few steps as follows:\n",
    "1. Read the raw data file\n",
    "2. Lowercase all the letters. If we do not do this, there will be multiple instances of the same word that appear in our dictionary which will affect the probability of those words and this is not suitable. Thus we lowercase all the words in order to have a clean and effective dictionary.\n",
    "3. Replace numbers with blank spaces\n",
    "4. Replace punctuations with blank spaces\n",
    "5. Tokenize the text into words\n",
    "6. Lemmatize the tokens. Lemmatization will enable for words which do not have the same root to be grouped together in order for them to be processed as one item. Stemming is the base action for lemmatization. As opposed to stemming, lemmatization does not simply chop off inflections. Instead it uses lexical knowledge bases to get the correct base forms of words. It is a good practice to do stemming or lemmatization as they will group the similar words together and help us reach better probabilities and eventually a better accuracy.\n",
    "These steps are done in order to extract only the words and not punctuations and numbers; because only the words have value for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tagger(nltk_tag):\n",
    "  if nltk_tag.startswith('J'):\n",
    "    return wordnet.ADJ\n",
    "  elif nltk_tag.startswith('V'):\n",
    "    return wordnet.VERB\n",
    "  elif nltk_tag.startswith('N'):\n",
    "    return wordnet.NOUN\n",
    "  elif nltk_tag.startswith('R'):\n",
    "    return wordnet.ADV\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "def preprocess(text):\n",
    "    new_text = str(text).lower()\n",
    "    new_text = new_text.strip()\n",
    "    new_text = re.sub(r\"\\d+\", \" \", new_text)\n",
    "    for ch in string.punctuation:\n",
    "        new_text = new_text.replace(ch, \" \")\n",
    "    new_text = ' '.join(new_text.split())\n",
    "    nt = [w for w in new_text.split() if w not in stop_words]\n",
    "#     stemmer = PorterStemmer()\n",
    "#     stemmed_nt = [stemmer.stem(w) for w in nt]\n",
    "    nltk_tagged = nltk.pos_tag(nt)\n",
    "    wn_tagged = map(lambda x: (x[0], tagger(x[1])), nltk_tagged)\n",
    "    lemmatized_nt = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for w, tag in wn_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_nt.append(w)\n",
    "        else:\n",
    "            lemmatized_nt.append(lemmatizer.lemmatize(w, tag))\n",
    "    return ' '.join(lemmatized_nt)\n",
    "\n",
    "dataset = pd.read_csv('data.csv')\n",
    "dataset.short_description.fillna(dataset.headline, inplace=True)\n",
    "dataset['short_description'] = dataset['short_description'].apply(preprocess)\n",
    "dataset['headline'] = dataset['headline'].apply(preprocess)\n",
    "dataset['authors'] = dataset['authors'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to split our dataset into two sets of training and evaluation data which we will use as both to train and extract a model and to evaluate the extracted model to see if it is good. We use a split factor of 0.8 for train data versus the split factor of 0.2 for evaluation data. We also do this split for each category on itself and not the dataset as a whole. This has to be done in order to preserve the distribution of each category in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(df, frac=0.8, seed=200):\n",
    "    travel_df = df[df['category'] == \"TRAVEL\"]\n",
    "    business_df = df[df['category'] == \"BUSINESS\"]\n",
    "    style_df = df[df['category'] == \"STYLE & BEAUTY\"]\n",
    "    travel_train = travel_df.sample(frac=frac,random_state=seed)\n",
    "    travel_test = travel_df.drop(travel_train.index)\n",
    "    business_train = business_df.sample(frac=frac,random_state=seed)\n",
    "    business_test = business_df.drop(business_train.index)\n",
    "    style_train = style_df.sample(frac=frac,random_state=seed)\n",
    "    style_test = style_df.drop(style_train.index)\n",
    "    return business_test, business_train, style_test, style_train, travel_test, travel_train\n",
    "business_test, business_train, style_test, style_train, travel_test, travel_train = split_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Phase 1\n",
    "The initial step in this phase is to extract all the words in our dataset. I use a combination of `headline`, `authors`, and `short_desctiption` in order to reach a better accuracy in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_words = set()\n",
    "\n",
    "def extract_words(df):\n",
    "    word_set = set()\n",
    "    for index, row in df.iterrows():\n",
    "        words = row['short_description'].split() + row['headline'].split() + row['authors'].split()\n",
    "        for w in words:\n",
    "            word_set.add(w)\n",
    "    return word_set\n",
    "all_words = all_words.union(extract_words(travel_train))\n",
    "all_words = all_words.union(extract_words(business_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we count the number of times that each word in our dictionary has been repeated in each category. This helps us calculate the conditional probabilities for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_occurences(df):\n",
    "    extraced = {}\n",
    "    for index, row in df.iterrows():\n",
    "        words = row['short_description'].split() + row['headline'].split() + row['authors'].split()\n",
    "        for w in words:\n",
    "            if w in extraced:\n",
    "                extraced[w] = extraced[w] + 1\n",
    "            else:\n",
    "                extraced[w] = 1\n",
    "\n",
    "    return extraced\n",
    "\n",
    "travel_dict = extract_occurences(travel_train)\n",
    "business_dict = extract_occurences(business_train)\n",
    "travel_num_of_occurences = sum(travel_dict.values())\n",
    "business_num_of_occurences = sum(business_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate the class prior probability and likelihood. The prior probabilities denoted by `P_travel` and `P_business` represent the probability of a given news in the dataset to be of the Travel or the Business category and is calculated by dividing the number of news with that category to the number of the news with all categories.\\\n",
    "The likelihood is a conditional probability that shows the probability of a word occuring in a certain class. It's calculated by dividing the number of times it has occured in a certain category to the sum of total number of all words' occurences and the number of words in our dictionaty.\\\n",
    "The predictor prior probability is not needed to be calculated because we compare the posterior probabilities with the same predictor probabilty, thus there is no need to calculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "P_travel = travel_train.shape[0] / (travel_train.shape[0] + business_train.shape[0])\n",
    "P_business = business_train.shape[0] / (travel_train.shape[0] + business_train.shape[0])\n",
    "\n",
    "P_words_in_travel = {}\n",
    "P_words_in_business = {}\n",
    "for word in all_words:\n",
    "    P_words_in_travel[word] = 1 / (travel_num_of_occurences + len(all_words))\n",
    "    P_words_in_business[word] = 1 / (business_num_of_occurences + len(all_words))\n",
    "\n",
    "for key, value in travel_dict.items():\n",
    "    P_words_in_travel[key] = (value + 1) / (travel_num_of_occurences + len(all_words))\n",
    "\n",
    "for key, value in business_dict.items():\n",
    "    P_words_in_business[key] = (value + 1) / (business_num_of_occurences + len(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to calculate the posterior probabilities for each news in the evaluation set. Using the naive bayes method, we assume that all the features are independent of each other, thus we multiply all the conditional probabilities for each word in the news in relation to each category and the resulting class is the category with the highest probability calculated through naive bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def classify(row):\n",
    "    text = row[1] + \" \" + row[4] + \" \" + row[6]\n",
    "    known_tokens = [w for w in text.split() if w in all_words]\n",
    "    travel_cat_prob = P_travel\n",
    "    business_cat_prob = P_business\n",
    "    for token in known_tokens:\n",
    "\n",
    "        travel_cat_prob = travel_cat_prob * P_words_in_travel[token]\n",
    "        business_cat_prob = business_cat_prob * P_words_in_business[token]\n",
    "\n",
    "    if business_cat_prob > travel_cat_prob:\n",
    "        return \"BUSINESS\"\n",
    "    else:\n",
    "        return \"TRAVEL\"\n",
    "\n",
    "travel_test['classified_as'] = travel_test.apply(lambda x: classify(x), axis=1)\n",
    "business_test['classified_as'] = business_test.apply(lambda x: classify(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the evaluation metrics are calculated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9758426966292135 0.9438727782974743\n",
      "0.9666110183639399 0.9591254752851711\n",
      "0.9638469638469639\n"
     ]
    }
   ],
   "source": [
    "classified_tests = travel_test.append(business_test)\n",
    "correct_detected_travel = travel_test[travel_test['classified_as'] == travel_test['category']].shape[0]\n",
    "correct_detected_business = business_test[business_test['classified_as'] == business_test['category']].shape[0]\n",
    "travel_count = travel_test.shape[0]\n",
    "business_count = business_test.shape[0]\n",
    "detected_travel = classified_tests[classified_tests['classified_as'] == \"TRAVEL\"].shape[0]\n",
    "detected_business = classified_tests[classified_tests['classified_as'] == \"BUSINESS\"].shape[0]\n",
    "\n",
    "recall_travel = correct_detected_travel / travel_count\n",
    "recall_business = correct_detected_business / business_count\n",
    "\n",
    "precision_travel = correct_detected_travel / detected_travel\n",
    "precision_business = correct_detected_business / detected_business\n",
    "\n",
    "accuracy = (correct_detected_travel + correct_detected_business) / classified_tests.shape[0]\n",
    "print(recall_travel, recall_business)\n",
    "print(precision_travel, precision_business)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<table align=\"center\">\n",
    "  <tr align=\"center\">\n",
    "    <th>Phase 1</th>\n",
    "    <th>Travel</th>\n",
    "    <th>Business</th>\n",
    "  </tr>\n",
    "  <tr align=\"center\">\n",
    "    <td>Recall</td>\n",
    "    <td>0.9758426966292135</td>\n",
    "    <td>0.9438727782974743</td>\n",
    "  </tr>\n",
    "  <tr align=\"center\">\n",
    "    <td>Precision</td>\n",
    "    <td>0.9666110183639399</td>\n",
    "    <td>0.9591254752851711</td>\n",
    "  </tr>\n",
    "  <tr align=\"center\">\n",
    "    <td>Accuracy</td>\n",
    "    <td align=\"center\" colspan=\"2\">0.9638469638469639</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Phase 2\n",
    "We repeat all the above steps in order to train a new model based on all three categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_words = all_words.union(extract_words(style_train))\n",
    "style_dict = extract_occurences(style_train)\n",
    "style_num_of_occurences = sum(style_dict.values())\n",
    "\n",
    "P_style = style_train.shape[0] / (style_train.shape[0] + business_train.shape[0] + travel_train.shape[0])\n",
    "P_travel = travel_train.shape[0] / (style_train.shape[0] + business_train.shape[0] + travel_train.shape[0])\n",
    "P_business = business_train.shape[0] / (style_train.shape[0] + business_train.shape[0] + travel_train.shape[0])\n",
    "\n",
    "P_words_in_travel = {}\n",
    "P_words_in_business = {}\n",
    "P_words_in_style = {}\n",
    "for word in all_words:\n",
    "    P_words_in_travel[word] = 1 / (travel_num_of_occurences + len(all_words))\n",
    "    P_words_in_business[word] = 1 / (business_num_of_occurences + len(all_words))\n",
    "    P_words_in_style[word] = 1 / (style_num_of_occurences + len(all_words))\n",
    "\n",
    "for key, value in travel_dict.items():\n",
    "    P_words_in_travel[key] = (value + 1) / (travel_num_of_occurences + len(all_words))\n",
    "\n",
    "for key, value in business_dict.items():\n",
    "    P_words_in_business[key] = (value + 1) / (business_num_of_occurences + len(all_words))\n",
    "\n",
    "for key, value in style_dict.items():\n",
    "    P_words_in_style[key] = (value + 1) / (style_num_of_occurences + len(all_words))\n",
    "    \n",
    "def new_classify(row):\n",
    "    text = row[1] + \" \" + row[4] + \" \" + row[6]\n",
    "    known_tokens = [w for w in text.split() if w in all_words]\n",
    "    travel_cat_prob = P_travel\n",
    "    business_cat_prob = P_business\n",
    "    style_cat_prob = P_style\n",
    "    for token in known_tokens:\n",
    "        style_cat_prob = style_cat_prob * P_words_in_style[token]\n",
    "        travel_cat_prob = travel_cat_prob * P_words_in_travel[token]\n",
    "        business_cat_prob = business_cat_prob * P_words_in_business[token]\n",
    "\n",
    "    if business_cat_prob > travel_cat_prob and business_cat_prob > style_cat_prob:\n",
    "        return \"BUSINESS\"\n",
    "    if travel_cat_prob > business_cat_prob and travel_cat_prob > style_cat_prob:\n",
    "        return \"TRAVEL\"\n",
    "    if style_cat_prob > business_cat_prob and style_cat_prob > travel_cat_prob:\n",
    "        return \"STYLE & BEAUTY\"\n",
    "\n",
    "travel_test['classified_as'] = travel_test.apply(lambda x: new_classify(x), axis=1)\n",
    "business_test['classified_as'] = business_test.apply(lambda x: new_classify(x), axis=1)\n",
    "style_test['classified_as'] = style_test.apply(lambda x: new_classify(x), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the evaluation metrics are calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9651685393258427 0.9242282507015903 0.9550949913644214\n",
      "0.9403393541324576 0.940952380952381 0.970743124634289\n",
      "0.9518098560837331\n"
     ]
    }
   ],
   "source": [
    "classified_tests = travel_test.append(business_test).append(style_test)\n",
    "correct_detected_travel = travel_test[travel_test['classified_as'] == travel_test['category']].shape[0]\n",
    "correct_detected_business = business_test[business_test['classified_as'] == business_test['category']].shape[0]\n",
    "correct_detected_style = style_test[style_test['classified_as'] == style_test['category']].shape[0]\n",
    "travel_count = travel_test.shape[0]\n",
    "business_count = business_test.shape[0]\n",
    "style_count = style_test.shape[0]\n",
    "detected_travel = classified_tests[classified_tests['classified_as'] == \"TRAVEL\"].shape[0]\n",
    "detected_business = classified_tests[classified_tests['classified_as'] == \"BUSINESS\"].shape[0]\n",
    "detected_style = classified_tests[classified_tests['classified_as'] == \"STYLE & BEAUTY\"].shape[0]\n",
    "\n",
    "recall_travel = correct_detected_travel / travel_count\n",
    "recall_business = correct_detected_business / business_count\n",
    "recall_style = correct_detected_style / style_count\n",
    "\n",
    "precision_travel = correct_detected_travel / detected_travel\n",
    "precision_business = correct_detected_business / detected_business\n",
    "precision_style = correct_detected_style / detected_style\n",
    "\n",
    "accuracy = (correct_detected_travel + correct_detected_business + correct_detected_style) / classified_tests.shape[0]\n",
    "\n",
    "print(recall_travel, recall_business, recall_style)\n",
    "print(precision_travel, precision_business, precision_style)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<table align=\"center\">\n",
    "  <tr align=\"center\">\n",
    "    <th>Phase 2</th>\n",
    "    <th>Travel</th>\n",
    "    <th>Business</th>\n",
    "    <th>Style & Beauty</th>\n",
    "  </tr>\n",
    "  <tr align=\"center\">\n",
    "    <td>Recall</td>\n",
    "    <td>0.9651685393258427</td>\n",
    "    <td>0.9242282507015903</td>\n",
    "    <td>0.9550949913644214</td>\n",
    "  </tr>\n",
    "  <tr align=\"center\">\n",
    "    <td>Precision</td>\n",
    "    <td>0.9403393541324576</td>\n",
    "    <td>0.940952380952381</td>\n",
    "    <td>0.970743124634289</td>\n",
    "  </tr>\n",
    "  <tr align=\"center\">\n",
    "    <td>Accuracy</td>\n",
    "    <td align=\"center\" colspan=\"3\">0.9518098560837331</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Note: As there has been no huge difference between the percentages, I decided not to do any oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1718 37 25\n",
      "56 988 25\n",
      "53 25 1659\n"
     ]
    }
   ],
   "source": [
    "ss = style_test[style_test['classified_as'] == \"STYLE & BEAUTY\"].shape[0]\n",
    "st = style_test[style_test['classified_as'] == \"TRAVEL\"].shape[0]\n",
    "sb = style_test[style_test['classified_as'] == \"BUSINESS\"].shape[0]\n",
    "tt = travel_test[travel_test['classified_as'] == \"TRAVEL\"].shape[0]\n",
    "tb = travel_test[travel_test['classified_as'] == \"BUSINESS\"].shape[0]\n",
    "ts = travel_test[travel_test['classified_as'] == \"STYLE & BEAUTY\"].shape[0]\n",
    "bb = business_test[business_test['classified_as'] == \"BUSINESS\"].shape[0]\n",
    "bt = business_test[business_test['classified_as'] == \"TRAVEL\"].shape[0]\n",
    "bs = business_test[business_test['classified_as'] == \"STYLE & BEAUTY\"].shape[0]\n",
    "print(tt, tb, ts)\n",
    "print(bt, bb, bs)\n",
    "print(st, sb, ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The confusion matrix is a simple method to find the accuracy and correctness of a model. The confusion matrix in itself is not a performance measure as such, but almost all of the performance metrics are based on confusion matrix and the numbers inside it. All the above metrics can be calculated by this matrix. The diameter of this matrix denotes the number of correct classifications that our model has done. The other cells showcase the number of false classifications the model has done. Four types of cells reside in the matrix that tell us wether a classification has been true or false and if our classifier has classified them truly or falsely which result in true negatives, true positives, false negatives and false positives. The diameter is consisted of the first two categories. The cells to the top of diameter are false positives and the cells to the bottom of it are false negatives.\n",
    "\n",
    "<table>\n",
    "    <tr align=\"center\">\n",
    "        <td></td>\n",
    "        <td colspan=\"4\">Actual</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"4\">Predicted</td>\n",
    "        <td></td>\n",
    "        <td>TRAVEL</td>\n",
    "        <td>BUSINESS</td>\n",
    "        <td>STYLE & BEAUTY</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>TRAVEL</td>\n",
    "        <td>1718</td>\n",
    "        <td>37</td>\n",
    "        <td>25</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>BUSINESS</td>\n",
    "        <td>56</td>\n",
    "        <td>988</td>\n",
    "        <td>25</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>STYLE & BEAUTY</td>\n",
    "        <td>53</td>\n",
    "        <td>25</td>\n",
    "        <td>1659</td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final classification of test file is done via the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv('test.csv')\n",
    "test_dataset.dropna(subset=['short_description'], inplace=True)\n",
    "test_dataset['short_description'] = test_dataset['short_description'].apply(preprocess)\n",
    "test_dataset['headline'] = test_dataset['headline'].apply(preprocess)\n",
    "test_dataset['authors'] = test_dataset['authors'].apply(preprocess)\n",
    "test_dataset.head(10)\n",
    "\n",
    "def final_classify(row):\n",
    "    text = row[1] + \" \" + row[2] + \" \" + row[4]\n",
    "    known_tokens = [w for w in text.split() if w in all_words]\n",
    "    travel_cat_prob = P_travel\n",
    "    business_cat_prob = P_business\n",
    "    style_cat_prob = P_style\n",
    "    for token in known_tokens:\n",
    "        style_cat_prob = style_cat_prob * P_words_in_style[token]\n",
    "        travel_cat_prob = travel_cat_prob * P_words_in_travel[token]\n",
    "        business_cat_prob = business_cat_prob * P_words_in_business[token]\n",
    "\n",
    "    if business_cat_prob > travel_cat_prob and business_cat_prob > style_cat_prob:\n",
    "        return \"BUSINESS\"\n",
    "    if travel_cat_prob > business_cat_prob and travel_cat_prob > style_cat_prob:\n",
    "        return \"TRAVEL\"\n",
    "    if style_cat_prob > business_cat_prob and style_cat_prob > travel_cat_prob:\n",
    "        return \"STYLE & BEAUTY\"\n",
    "\n",
    "test_dataset['category'] = test_dataset.apply(lambda x: final_classify(x), axis=1)\n",
    "test_dataset[['index', 'category']].to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#### Questions\n",
    "1. Lemmatization will enable for words which do not have the same root to be grouped together in order for them to be processed as one item. Stemming is the base action for lemmatization. As opposed to stemming, lemmatization does not simply chop off inflections. Instead it uses lexical knowledge bases to get the correct base forms of words. Stemming is faster and simpler because it does not check if the resulting word has a meaning or not. It is a good practice to do stemming or lemmatization as they will group the similar words together and help us reach better probabilities and eventually a better accuracy. These steps are done in order to extract only the words and not punctuations and numbers; because only the words have value for us. I used stemmer but there was not a huge difference in the accuracy. However lemmatization worked slightly (~%0.1) better for me.\n",
    "2. A problem with calculating word occurrences is that highly repeated words start to dominate in the document with larger scores, but may not contain as much informational content to the model as rarer but more domain specific words. It’s a score to highlight each word’s relevance in the entire document. The calculations are done via IDF =Log\\[(Number of documents) / (Number of documents containing the word)\\] and TF = (Number of repetitions of word in a document) / (# of words in a document). So if a word is repeated a lot but is present in more documents, then it's not very important. But if a word is repeated in less documents but has more occurrences, then it could be more helpful and has more IDF. So instead of word counts, we shall use the terms in the naive bayes calculations and there will be higher probabilities for a word that is more important in a certain class.\n",
    "3. If we have a high precision but low recall, then we may be missing out on a lot of the desired class that we need to identify. We may have a low number of fales positives, meaning that most of the labeled data is true but we have missed a lot of the positives, leading to a low recall. For example it is important not to miss the spam emails. We can have a high precision by not labeling most of the emails as spam and therefore we will have a low recall because of a large number of false negatives. This will affect in a bad way if we need to sensitively do the classification.\n",
    "4. This way, there will be no difference between a word which is not in that categoty but is in other categories and a word which has occurred only once. But Tabriz is more related to the Travel category but we are not making any importance to it. As I have used the multinomial naive bayes, there will be no difference between the two mentioned conditions, thus it will be treated as a word not belonging to the travel category. But if we ignore a word that is not in a category and we multiply the calculations for travel category with the probability, it will lower this probability in comparison to the others and therefore there will be less chance for the Travel category and it's a complete false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (CA3)",
   "language": "python",
   "name": "pycharm-2ee6626b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
